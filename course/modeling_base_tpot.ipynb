{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the base classifier\n",
    "\n",
    "Create a base line, how good can we predict without deep-learning? Les create a base classifier using tpot out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\") # go to parent dir\n",
    "# from customFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gus/workspaces/wpy/venvs/mathor/lib/python3.6/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tpot import TPOTClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from data import read_crop_list, load_structured_sample\n",
    "from eval import eval_model\n",
    "from training import create_training_folder\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_crops, vocab = read_crop_list()\n",
    "sample = load_structured_sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21961, 8)\n",
      "(9412, 8)\n"
     ]
    }
   ],
   "source": [
    "y = sample[:, 11]\n",
    "X = sample[:, 3:11]\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of Model Performance Indicator\n",
    "As a performance metric, the f1-score on the test set has been used, this metric balances between precision and recall and makes it more robust in unbalanced datasets. This is the case in our project, we have crops with extreamly high frequencies comprared to less represented cultives and we want to balance the precision and recall of the classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A stats based model\n",
    "We can make a very basic model just using the crop code population distribution and the the prior knowledge that we have about the data and the context:\n",
    "1. Some categories have extreamly high frequence, so if we have to guess a category, we can just use the main category as a base prediction.\n",
    "2. Some crops are very static and usually don't change over the years, so we can assume that next year prediction will be the previous year crop code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    code                  description  is_crop  idx\n",
      "0      1                        TRIGO        1    0\n",
      "1      3                URBANO-VIALES        0    1\n",
      "2      4                         MAIZ        1    2\n",
      "3      5                       CEBADA        1    3\n",
      "4      8               OTROS CEREALES        1    4\n",
      "5      9                     ROQUEDOS        0    5\n",
      "6     20                SUELO DESNUDO        1    6\n",
      "7     33                      GIRASOL        1    7\n",
      "8     35                        COLZA        1    8\n",
      "9     39      OTRAS LEGUMINOSAS GRANO        1    9\n",
      "10    40                    GUISANTES        1   10\n",
      "11    60                      ALFALFA        1   11\n",
      "12    61                   FORRAJERAS        1   12\n",
      "13    80  OTROS CULTIVOS INDUSTRIALES        1   13\n",
      "14    82                    REMOLACHA        1   14\n",
      "16   100                       VIÑEDO        1   16\n",
      "17   101                       OLIVAR        1   17\n",
      "18   110                    HORTICOLA        1   18\n",
      "19   177                   AROMATICAS        1   19\n",
      "20   181                     FRUTALES        1   20\n",
      "21   183             FRUTALES CASCARA        1   21\n",
      "22   200                     PASTIZAL        0   22\n",
      "23   201                     MATORRAL        0   23\n",
      "24   202                    CONIFERAS        0   24\n",
      "25   203       FRONDOSAS CADUCIFOLIAS        0   25\n",
      "26   204     FRONDOSAS SIEMPRE VERDES        0   26\n",
      "27   255                  LAMINA AGUA        0   27\n"
     ]
    }
   ],
   "source": [
    "print(df_crops)\n",
    "# For static crop or usages, use the value from the last year\n",
    "static_crop_codes = [1,5,16,17,20,21,25,26,27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop codes and frequencies\n",
      "0      474535\n",
      "3      794460\n",
      "21    1019903\n",
      "6     1243298\n",
      "7     1378289\n",
      "4     1492494\n",
      "22    1581247\n",
      "24    1667043\n",
      "25    1730292\n",
      "11    1778045\n",
      "2     1819273\n",
      "1     1856963\n",
      "12    1883033\n",
      "15    1908758\n",
      "23    1929418\n",
      "10    1940491\n",
      "9     1951309\n",
      "16    1958146\n",
      "19    1964575\n",
      "14    1969813\n",
      "8     1974185\n",
      "17    1977818\n",
      "20    1981128\n",
      "26    1983104\n",
      "5     1984153\n",
      "13    1984471\n",
      "dtype: int64\n",
      "\n",
      "shapes: (1984471,) == (1984471,)\n"
     ]
    }
   ],
   "source": [
    "data_file = \"/media/data/projects/crophisto/data.npy\"\n",
    "data_orig = np.load(data_file)\n",
    "y_orig = data_orig[:, 11]\n",
    "X_orig = data_orig[:, :11]\n",
    "\n",
    "last_year = X_orig[:,-1] # last year usage\n",
    "y_pred = np.zeros(shape = y_orig.shape)\n",
    "\n",
    "\n",
    "# create a CDF a cummulative function of the crop frequences\n",
    "cumulative_sum = pd.value_counts(last_year).cumsum()\n",
    "# maps each crop code to its frequence\n",
    "freq_map =cumulative_sum.to_dict()\n",
    "print(\"Crop codes and frequencies\")\n",
    "print(pd.Series(freq_map))\n",
    "\n",
    "# lest get the cummulative distrubition of data, so we randomly choose the crop code based on its frequence.\n",
    "max_value = pd.value_counts(last_year).sum()\n",
    "def get_random_crop(t=None):\n",
    "    \"\"\"\n",
    "    This function returns a random crop code using the crop distribution of the last year.\n",
    "    \"\"\"\n",
    "    value = randrange(max_value)\n",
    "    for k, crop_value in freq_map.items():\n",
    "        if value < crop_value:\n",
    "            return k\n",
    "    return k\n",
    "\n",
    "# Apply the function for all non static codes\n",
    "y_pred = np.array(list(map(get_random_crop, y_pred)))\n",
    "print(\"\\nshapes: {} == {}\".format(y_orig.shape, y_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of crops that vary from one year to the next: 62.66 %\n",
      "F1-score if we just take the last year crop: 0.3393251651296892\n",
      "F1-score if we use static and prob codes: 0.1298599394722416\n",
      "F1-score if we use the main category for non static crops : 0.154172731814438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "static_mask = np.isin(y_orig, static_crop_codes)\n",
    "# same as last year\n",
    "y_pred[static_mask] = last_year[static_mask]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# lest evaluate how good is this predictor using f1 socre\n",
    "\n",
    "print(\"Percentage of crops that vary from one year to the next: {} %\".format(round(100*np.count_nonzero(last_year-y_orig)/len(y_orig),2)))\n",
    "print(\"F1-score if we just take the last year crop: {}\".format(f1_score(y_orig, last_year, average=\"macro\")))\n",
    "print(\"F1-score if we use static and prob codes: {}\".format(f1_score(y_orig, y_pred, average=\"macro\")))\n",
    "most_freq_crop_code = 0\n",
    "y_pred[~static_mask] = most_freq_crop_code\n",
    "print(\"F1-score if we use the main category for non static crops : {}\".format(f1_score(y_orig, y_pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using the last year crop, we have a **0.34 f1-score**, not enough, it seems there's more variability than expected but it remarks the need of finding a proper classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TPOT to create a base model\n",
    "\n",
    "TPOT is a library that automates the phases of searching models and feature extraction, lets see what's the best model we can get using TOP out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=180.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.4706572541797421\n",
      "Generation 2 - Current best internal CV score: 0.4771235737178454\n",
      "Generation 3 - Current best internal CV score: 0.48062829150066666\n",
      "Generation 4 - Current best internal CV score: 0.48278584018315823\n",
      "Generation 5 - Current best internal CV score: 0.48278584018315823\n",
      "Best pipeline: ExtraTreesClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.8, min_samples_leaf=2, min_samples_split=12, n_estimators=100)\n",
      "Final score: 0.49752538168292254\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=30, verbosity=2, random_state=42, scoring=\"f1_macro\")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(\"Final score: {}\".format(tpot.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually measure the f1-score: 0.49752538168292254\n"
     ]
    }
   ],
   "source": [
    "folder = create_training_folder(\"tpot\",\"base\")\n",
    "\n",
    "df_crops, vocab = read_crop_list()\n",
    "crop_names = df_crops[\"description\"].values.tolist()\n",
    "crop_list = df_crops[\"code\"].values.tolist()\n",
    "\n",
    "tpot.export('{}/model_tpot.py'.format(folder))\n",
    "    \n",
    "# run prediction\n",
    "y_pred = tpot.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "print(\"Manually measure the f1-score: {}\".format(f1_score(y_test, y_pred, average=\"macro\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So TPOT has found a **ExtraTreesClassifier that could give us a 0.5 f1-score**, lets see if we can improve this using deep-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "738.722px",
    "left": "1371.45px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05555a8434ac47db900d16af3dde707e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "0cd712ee706949b784ae077ac2263367": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "17bad21d3981408d9c0fa2202824ed87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ac0bdca324b40f0a296579583e30ee0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e674563fd1294257a3a5853b07443c66",
       "style": "IPY_MODEL_fd5326dac12f43cf924c5a6a8f4769f8",
       "value": " 182/? [2:18:28&lt;00:00, 65.46s/pipeline]"
      }
     },
     "d24d3111f8c848d0981967841cab848f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "description": "Optimization Progress: ",
       "layout": "IPY_MODEL_0cd712ee706949b784ae077ac2263367",
       "max": 180,
       "style": "IPY_MODEL_05555a8434ac47db900d16af3dde707e",
       "value": 180
      }
     },
     "d2c01642586948d5961a5567ac040cce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d24d3111f8c848d0981967841cab848f",
        "IPY_MODEL_2ac0bdca324b40f0a296579583e30ee0"
       ],
       "layout": "IPY_MODEL_17bad21d3981408d9c0fa2202824ed87"
      }
     },
     "e674563fd1294257a3a5853b07443c66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd5326dac12f43cf924c5a6a8f4769f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
