{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Visualization and Quality Assessment\n",
    "\n",
    "As mention in the project intro, in the real project we have 2M samples but this data cannot be shared, I've extracted a 30M sample dataset that I used during the training process. \n",
    "\n",
    "The data exploration starts sampling the 2M dataset trying to keep the crop code distrutions, so the sample faithfully represents the original data. \n",
    "\n",
    "As you can see in the notebook, some of the crop categories has extreamly high apparition frequencies, to avoid problemas with the model, we oversample the less represented categories to keep a mininum sample size of 1000.\n",
    "One crop category has been discarted because there's no enough data in the samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/media/data/projects/crophisto/data.npy\"\n",
    "data = np.load(data_file)\n",
    "y = data[:, 11]\n",
    "X = data[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataste: 1984471\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in the dataste: {}\".format(y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 11)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# use StratifiedShuffleSplit from sklearn to keep data distributions\n",
    "\n",
    "sample_size = 15000\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_size, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "print(X_test.shape)\n",
    "print(y_test.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9685aa308c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have 15K dataset, lets see if the crop distributions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categories under represented\n",
    "categories = np.sort(np.unique(y_test))\n",
    "\n",
    "freq1, bins1 = np.histogram(y, bins=categories)\n",
    "freq2, bins2 = np.histogram(y_test, bins=categories)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].hist(y_test,)\n",
    "axs[0].set_title(\"original\")\n",
    "axs[1].hist(y,)\n",
    "axs[1].set_title(\"sampled\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the categories are clearly unbalanced. This may suppose that the classifier tends to learn more accurately the patterns that the majority crops have as output, but those of less frequency won't are not adequately represented in the classifier hypothesis.\n",
    "Although statistically this would make the classifier work \"well\", because in general terms it will give a good result if it hits the most representative classes, we are interested in that it is also capable of predicting other types of crops.\n",
    "\n",
    "There are different techniques to deal with unbalanced groups (undersampling, oversampling, smote,) a library that implements this type of techniques and is of great help is [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html).\n",
    "\n",
    "In this case, I have manually included a minimum of data for each category of crop. For each category, lest define the min number of samples as the 30% of the mean expected frequency, and calculate how many samples we have to add to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find categories with less that min_size\n",
    "num_categories = len(np.unique(y_test))\n",
    "mean_frequency = (100/num_categories)/100 # porc\n",
    "mean_support = round(sample_size * mean_frequency)\n",
    "# min_frequency = mean_frequency * 0.15\n",
    "min_frequency = mean_frequency * 0.30\n",
    "min_support = 1000 #round(sample_size * min_frequency)\n",
    "round(min_support)\n",
    "print(\"Mean support: {}, mean_frequency: {:.2%}\".format(mean_support, mean_frequency))\n",
    "print(\"Min support: {}, min_frequency: {:.2%}\".format(min_support, min_frequency))\n",
    "\n",
    "# get frequency per category\n",
    "categories = np.sort(np.unique(y_test))\n",
    "freq_df = pd.DataFrame(y_test)\n",
    "freq_df.columns = [\"code\"]\n",
    "freq = freq_df[\"code\"].value_counts().sort_index().values\n",
    "\n",
    "fig = freq_df[\"code\"].value_counts().sort_index().plot(kind='bar')\n",
    "fig.hlines(min_support,0,max(y_test), colors=\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of samples per category.\")\n",
    "print(categories)\n",
    "print(freq)\n",
    "print(\"Undersampled categories that need additional data.\")\n",
    "\n",
    "needed = (freq-min_support)\n",
    "needed\n",
    "needed[needed >0] = 0\n",
    "needed = -1*needed\n",
    "print(categories)\n",
    "\n",
    "print(\"needed: \\n{}\".format(needed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets iterate through each category addind the needed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "needed = (freq-min_support)\n",
    "needed\n",
    "needed[needed >0] = 0\n",
    "needed = -1*needed\n",
    "print(\"Initial dataset shape: {} \".format(X.shape))\n",
    "y_over = y_test.copy()\n",
    "X_over = X_test.copy()\n",
    "\n",
    "data_copy = data[:, 3:11]\n",
    "\n",
    "for index, needed_for_category in enumerate(needed):\n",
    "    if needed_for_category == 0:\n",
    "        continue\n",
    "    category_code = categories[index]\n",
    "    \n",
    "    print(\"Adding {} samples for category {}\".format(needed_for_category, category_code))\n",
    "    # filter data for this category\n",
    "    cat_mask = (y_train == category_code)\n",
    "    \n",
    "    # define the lower bound as the limit of samples to get from this category\n",
    "    bound = min(needed_for_category, len(cat_mask == True))\n",
    "    cat_y = y_train[cat_mask][:bound]\n",
    "    y_over = np.hstack([y_over,cat_y])\n",
    "\n",
    "    cat_X = X_train[cat_mask,:][:bound,:]\n",
    "    X_over = np.vstack([X_over,cat_X])\n",
    "\n",
    "\n",
    "dataset = np.hstack([X_over, y_over.reshape(-1,1)])\n",
    "print(\"Final dataset shape: {}\".format(dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets take a look at the final sample vs original distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequency per category\n",
    "categories = np.sort(np.unique(y_test))\n",
    "freq_df = pd.DataFrame(y_test)\n",
    "freq_df.columns = [\"code\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].set_title(\"original sampled data\")\n",
    "axs[1].set_title(\"oversampled data\")\n",
    "pd.value_counts(y).sort_index().plot(ax=axs[0],kind='bar')\n",
    "pd.value_counts(y_over).sort_index().plot(ax=axs[1],kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for some categories we don't haven enough data to get a mininum sample size, we are going to turn out samples that has category 255 in any year or in the y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 255\n",
    "dataset = dataset[~ (dataset[:]==code).any(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data\n",
    "\n",
    "np.save(\"../resources/data_sampled\", dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "512.323px",
    "left": "912.323px",
    "right": "20px",
    "top": "120px",
    "width": "352.323px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
