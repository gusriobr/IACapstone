{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the base classifier\n",
    "\n",
    "Create a base line, how good can we predict without deep-learning? Les create a base classifier using tpot out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\") # go to parent dir\n",
    "# from customFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gus/workspaces/wpy/venvs/mathor/lib/python3.6/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tpot import TPOTClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from data import read_crop_list, load_structured_sample\n",
    "from eval import eval_model\n",
    "from training import create_training_folder\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: (30941, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_crops, vocab = read_crop_list()\n",
    "sample = load_structured_sample()\n",
    "\n",
    "print(\"Sample size: {}\".format(sample.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21658, 8)\n",
      "(9283, 8)\n"
     ]
    }
   ],
   "source": [
    "y = sample[:, 11]\n",
    "X = sample[:, 3:11]\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of Model Performance Indicator\n",
    "As a performance metric, the f1-score on the test set has been used, this metric balances between precision and recall and makes it more robust in unbalanced datasets. This is the case in our project, we have crops with extreamly high frequencies comprared to less represented cultives and we want to balance the precision and recall of the classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A stats based model\n",
    "We can make a very basic model just using the population distrubition of our data the prior knowledge that we have about it.\n",
    "1. Some categories have extreamly high frequence, so if we have to guess a category, we can just use the main category as a base prediction.\n",
    "2. Some crops are very static and usually don't change over the years, so we can assume that next year prediction will be the previous year crop code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop codes and frequencies\n",
      "1       6330\n",
      "5      10717\n",
      "20     13461\n",
      "200    15855\n",
      "203    17458\n",
      "33     18995\n",
      "8      20270\n",
      "4      21382\n",
      "201    22468\n",
      "3      23464\n",
      "60     24453\n",
      "100    25425\n",
      "204    26392\n",
      "101    27351\n",
      "183    28126\n",
      "202    28895\n",
      "181    29493\n",
      "61     29873\n",
      "110    30196\n",
      "40     30377\n",
      "9      30552\n",
      "39     30712\n",
      "82     30842\n",
      "35     30931\n",
      "80     30941\n",
      "dtype: int64\n",
      "\n",
      "shapes: (30941,) == (30941,)\n"
     ]
    }
   ],
   "source": [
    "last_year = X[:,-1] # last year usage\n",
    "y_pred = np.zeros(shape = y.shape)\n",
    "\n",
    "\n",
    "# create a CDF a cummulative function of the crop frequences\n",
    "cumulative_sum = pd.value_counts(last_year).cumsum()\n",
    "# maps each crop code to its frequence\n",
    "freq_map =cumulative_sum.to_dict()\n",
    "print(\"Crop codes and frequencies\")\n",
    "print(pd.Series(freq_map))\n",
    "\n",
    "# lest get the cummulative distrubition of data, so we randomly choose the crop code based on its frequence.\n",
    "max_value = pd.value_counts(last_year).sum()\n",
    "def get_random_crop(t=None):\n",
    "    \"\"\"\n",
    "    This function returns a random crop code using the crop distribution of the last year.\n",
    "    \"\"\"\n",
    "    value = randrange(max_value)\n",
    "    for k, crop_value in freq_map.items():\n",
    "        if value < crop_value:\n",
    "            return k\n",
    "    return k\n",
    "\n",
    "# Apply the function for all non static codes\n",
    "y_pred = np.array(list(map(get_random_crop, y_pred)))\n",
    "print(\"\\nshapes: {} == {}\".format(y.shape, y_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score if we just take the last year crop: 0.3505316223774698\n",
      "F1-score if we use static and prob codes: 0.23371528564154648\n"
     ]
    }
   ],
   "source": [
    "# For static crop or usages, use the value from the last year\n",
    "static_crop_codes = [204,203,202,183,181,101,100,9,3]\n",
    "\n",
    "static_mask = np.isin(y, static_crop_codes)\n",
    "# same as last year\n",
    "y_pred[static_mask] = last_year[static_mask]\n",
    "y_pred[~static_mask] = 1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y, y_pred, average=\"macro\")\n",
    "# lest evaluate how good is this predictor using f1 socre\n",
    "\n",
    "print(\"F1-score if we just take the last year crop: {}\".format(f1_score(y, last_year, average=\"macro\")))\n",
    "print(\"F1-score if we use static and prob codes: {}\".format(f1_score(y, y_pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using the last year crop, we have a **0.23 f1-score**, not enough, it seems there's more variability than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TPOT to create a base model\n",
    "\n",
    "TPOT is a library that automates the phases of searching models and feature extraction, lets see what's the best model we can get using TOP out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25167adc177c466691428e9f3b2c5c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=180.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=30, verbosity=2, random_state=42, scoring=\"f1_macro\")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(\"Final score: {}\".format(tpot.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = create_training_folder(\"tpot\")\n",
    "eval_model(folder, y_test, y_hat, crop_list, crop_names)\n",
    "# save model\n",
    "# model_folder = '{}/model'.format(folder)\n",
    "# model.save(model_folder)\n",
    "tpot.export('{}/model_tpot.py'.format(folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "738.722px",
    "left": "1371.45px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
