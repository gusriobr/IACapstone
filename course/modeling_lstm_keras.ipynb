{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a crop serie predictor using Keras\n",
    "Base model has perfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0e5e67c13bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_crop_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_structured_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_model_one_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.models import Model\n",
    "\n",
    "from data import read_crop_list, load_structured_sample\n",
    "from eval import eval_model_one_hot\n",
    "from metrics import f1, f1_loss\n",
    "from models import model_stack_lstm, model_Conv1D, model_Conv1D_LSTM\n",
    "from training import train, create_training_folder\n",
    "from keras.utils import plot_model\n",
    "from training import train, create_training_folder, create_callbacks, step_decay_exp\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Embedding, LSTM, BatchNormalization, Dense, Conv1D, GlobalAveragePooling1D, \\\n",
    "    Dropout, Input, Permute, Concatenate\n",
    "\n",
    "from training import train, create_training_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_name):\n",
    "    loaded_model = keras.models.load_model(file_name)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import train, create_training_folder\n",
    "\n",
    "tf.keras.utils.get_custom_objects()\n",
    "tf.keras.utils.get_custom_objects()['f1'] = f1\n",
    "tf.keras.utils.get_custom_objects()['f1_loss'] = f1_loss\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_crops, vocab = read_crop_list()\n",
    "\n",
    "sample = load_structured_sample()\n",
    "# sample = load_undersampled_data()\n",
    "print(\"Using sample size: {}\".format(sample.shape))\n",
    "\n",
    "y = sample[:, 8]\n",
    "X = sample[:, 0:8]\n",
    "sequence_length = X.shape[-1]\n",
    "\n",
    "# X = one_hot_encoding_X(X, vocab_size=vocab_size)\n",
    "y = to_categorical(y)\n",
    "# random train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# y_train = to_categorical(y_train, num_classes=vocab_size)\n",
    "# y_test = to_categorical(y_test, num_classes=vocab_size)\n",
    "\n",
    "print(np.unique(y_train).shape)\n",
    "\n",
    "print(\"===Train/test size: {}, {}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_names = df_crops[\"description\"].values.tolist()\n",
    "crop_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(X)\n",
    "np.unique(y)\n",
    "\n",
    "df_crops\n",
    "\n",
    "model = load_model('../results/20200719_181629_lstm_64_64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "class_test = np.argmax(y_test, axis=1)\n",
    "class_predicted = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# eval_model(folder, class_predicted, class_test, classes, labels)\n",
    "np.unique(class_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 8\n",
    "vocab_size, embedding_size = (28, 5)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "layer_size = 128\n",
    "\n",
    "input = Input(shape=(sequence_length,)) \n",
    "\n",
    "# embeddings = Embedding(vocab_size, embedding_size,input_length=sequence_length)\n",
    "\n",
    "embeddings = Embedding(output_dim=embedding_size, input_dim=vocab_size, input_length=sequence_length)(input) \n",
    "\n",
    "input = Input(shape=(sequence_length,))\n",
    "\n",
    "embeddings = Embedding(output_dim=embedding_size, input_dim=vocab_size, input_length=sequence_length)(input)\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(Permute((2, 1)))\n",
    "\n",
    "    \n",
    "conv1 = Conv1D(filters=32, kernel_size=2, padding=\"same\", strides=1, activation='relu')(embeddings)\n",
    "conv2 = Conv1D(filters=32, kernel_size=2, padding=\"same\", strides=1, activation='relu')(conv1)\n",
    "# # conv3 = Conv1D(filters=32, kernel_size=7, padding=\"same\", strides=1, activation='relu')(embeddings)\n",
    "# lstm1 = LSTM(64, dropout=0.3, recurrent_dropout=0.2, return_sequences = True)(embeddings)\n",
    "\n",
    "concat = Concatenate(axis=2)([conv1, embeddings])\n",
    "lstm = LSTM(layer_size, dropout=0.3, recurrent_dropout=0.2)(concat)\n",
    "\n",
    "output = Dense(vocab_size, activation='relu')(lstm)\n",
    "model = Model(input, output, name=\"conv-lstm_parallel\")\n",
    "\n",
    "plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "sequence_length = 8\n",
    "\n",
    "vocab_size, embedding_size = (28, 5)\n",
    "layer_size = (64, 32)\n",
    "final_activation = \"relu\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size,\n",
    "                    input_length=sequence_length))\n",
    "return_sequences = True\n",
    "for i, units in enumerate(layer_size):\n",
    "    if i == len(layer_size) - 1:  # last layer\n",
    "        return_sequences = False\n",
    "    model.add(LSTM(units, dropout=0.3, recurrent_dropout=0.4,\n",
    "                   activation='relu', return_sequences=return_sequences))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(vocab_size, activation=final_activation))\n",
    "tag = \"conv_lstm_128_64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'loss': f1_loss,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'metrics': [f1],\n",
    "    'run_eagerly': False\n",
    "}\n",
    "\n",
    "model.compile(**training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "\n",
    "folder = create_training_folder(tag)\n",
    "\n",
    "lr_scheduller = tf.keras.callbacks.LearningRateScheduler(step_decay_exp)\n",
    "#lr_scheduller = tf.keras.callbacks.ReduceLROnPlateau(step_decay_exp)\n",
    "lr_scheduller = None\n",
    "callbacks = create_callbacks(folder, tensor_board=True, monitor_metric=\"val_f1\", monitor_mode=\"max\",\n",
    "                     lr_scheduller=lr_scheduller)\n",
    "\n",
    "\n",
    "train(model, X_train, y_train, X_test, y_test, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "# save model\n",
    "model_folder = '{}/model'.format(folder)\n",
    "model.save(model_folder)\n",
    "\n",
    "# evaluate\n",
    "y_hat = model.predict(X_test)\n",
    "eval_model_one_hot(folder, y_test, y_hat, crop_list, crop_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "738.722px",
    "left": "1371.45px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
